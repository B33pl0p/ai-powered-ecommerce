{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "from concurrent.futures import ProcessPoolExecutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"data/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/biplop/majorproject/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/biplop/majorproject/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained ResNet-50 model\n",
    "model = resnet50(pretrained=True)\n",
    "model = model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Remove the final fully connected layer to use ResNet as a feature extractor\n",
    "model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "\n",
    "# Image preprocessing (same as used for ImageNet pretraining)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from an image using ResNet-50\n",
    "def extract_features(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        features = model(image)\n",
    "    return features.squeeze().numpy()  # Convert to NumPy array and remove batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  12%|█▏        | 12/100 [25:15<3:04:44, 125.96s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Function to process a batch of images\n",
    "def process_images_in_batches(image_paths):\n",
    "    image_feature_dict = {}\n",
    "    for image_path in image_paths:\n",
    "        image_name = os.path.basename(image_path)\n",
    "        feature_vector = extract_features(image_path)  # Feature extraction\n",
    "        image_feature_dict[image_name] = feature_vector\n",
    "    return image_feature_dict\n",
    "\n",
    "# List all image files in the folder\n",
    "image_list = os.listdir(image_folder)\n",
    "image_paths = [os.path.join(image_folder, img_name) for img_name in image_list]\n",
    "\n",
    "# Step 1: Batch processing without parallel execution\n",
    "batch_size = 500  # You can tune this for batch processing based on your system's resources\n",
    "\n",
    "# Dictionary to store all image names and feature vectors\n",
    "all_image_features = {}\n",
    "\n",
    "# Calculate total batches for progress tracking\n",
    "total_batches = len(image_paths) // batch_size + (1 if len(image_paths) % batch_size != 0 else 0)\n",
    "\n",
    "# Process the images in batches with a progress bar\n",
    "for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Processing batches\"):\n",
    "    batch = image_paths[i:i + batch_size]\n",
    "    batch_features = process_images_in_batches(batch)  # Process the batch\n",
    "    all_image_features.update(batch_features)  # Store the batch's results\n",
    "\n",
    "# Save the image name to feature vector mapping as a pickle file\n",
    "with open('image_feature_vectors.pkl', 'wb') as f:\n",
    "    pickle.dump(all_image_features, f)\n",
    "\n",
    "print(\"Feature extraction complete. All features saved to 'image_feature_vectors.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature vectors from the pickle file\n",
    "pickle_file_path = 'image_feature_vectors.pkl'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    image_features = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(image_features)} image features from the pickle file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract image names (keys) and feature vectors (values)\n",
    "image_names = list(image_features.keys())  # List of image names (primary keys)\n",
    "feature_vectors = np.array(list(image_features.values())).astype('float32')  # Feature vectors as numpy array\n",
    "\n",
    "print(f\"Extracted {len(image_names)} image names and corresponding feature vectors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set FAISS parameters for IVF index creation\n",
    "dimension = feature_vectors.shape[1]  # ResNet-50 outputs 2048-dimensional feature vectors\n",
    "nlist = 100  # Number of clusters for IVF (you can tune this based on dataset size)\n",
    "nprobe = 10  # Number of clusters to search (can be tuned for search performance)\n",
    "\n",
    "# Create the IVF index with Flat L2 distance and enable HNSW for quantization\n",
    "quantizer = faiss.IndexHNSWFlat(dimension, 32)  # Using HNSW for quantization\n",
    "index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_L2)  # IVF with L2 distance\n",
    "\n",
    "print(\"FAISS index and quantizer initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the FAISS index (required for IVF)\n",
    "index.train(feature_vectors)\n",
    "print(\"FAISS index training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add vectors to the FAISS index and create mappings\n",
    "faiss_id_to_image_name = {}\n",
    "\n",
    "index.add(feature_vectors)  # Add all the feature vectors to the FAISS index\n",
    "for i, image_name in enumerate(image_names):\n",
    "    faiss_id_to_image_name[i] = image_name  # Store the mapping between FAISS ID and image name\n",
    "\n",
    "print(f\"Added {len(image_names)} feature vectors to the FAISS index and created ID-to-image mappings.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
