{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd7220b-0b31-4c1b-aa85-cecc970a6916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfde00a3-ea5d-473b-a951-f5c5cafde255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import clip\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "#from google.colab import drive\n",
    "\n",
    "#loading CLIP MODEL and preprocessing function\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81aef450-bbb9-437c-a833-36cdf355f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/saurav/Documents'\n",
    "\n",
    "csv_file = DATA_PATH+'/required_dataset/styles2.csv'\n",
    "df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb4cc1ba-92d1-4bed-b04a-3a3ad210f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining image paths and files\n",
    "image_folder = DATA_PATH+'/required_dataset/images'\n",
    "embeddings = {}\n",
    "batch_size = 8\n",
    "total_rows = len(df)\n",
    "checkpoint_file = \"clip_embeddings_checkpoint.pkl\"\n",
    "final_embeddings = \"clip_embeddings_batch.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8721e367-a5f4-4602-8d4d-4f45f6aa9a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to combine text features from multiple columns\n",
    "def create_text_description(row):\n",
    "  columns = [\n",
    "      str(row['gender']),\n",
    "      str(row['masterCategory']),\n",
    "      str(row['subCategory']),\n",
    "      str(row['articleType']),\n",
    "      str(row['baseColour']),\n",
    "      str(row['season']),\n",
    "      str(row['year']),\n",
    "      str(row['usage']),\n",
    "      str(row['productDisplayName'])\n",
    "  ]\n",
    "  #concatinating all relevant columns into single description\n",
    "  #as join only supports string so converted all column values to str to avoid null and integer data types\n",
    "  return ' '.join(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36507d18-9287-4f16-9e99-eba009c5d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the checkpoint if exists\n",
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    processed_ids = set(embeddings.keys())  \n",
    "else:\n",
    "    embeddings = {}\n",
    "    processed_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b1d4b96-269c-4a64-a758-79236a3223dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"clip_embeddings_checkpoint.pkl\"\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb93ae4c-5d27-47d8-bee9-296f275efddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    processed_ids = set(embeddings.keys())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7514a90-79af-4638-bf65-535d30374100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 5556/5556 [02:12<00:00, 41.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings have been saved to clip_embeddings_batch.pkl\n"
     ]
    }
   ],
   "source": [
    "#FOR THE EXTRACTION OF TEXT AND IMAGE EMBEDDING\n",
    "\n",
    "# Process the data in batches\n",
    "for start_idx in tqdm(range(0, total_rows, batch_size)):\n",
    "    end_idx = min(start_idx + batch_size, total_rows)\n",
    "    batch = df.iloc[start_idx:end_idx]\n",
    "\n",
    "    #filter out rows whose IDs have already been processed\n",
    "    batch = batch[~batch['id'].astype(str).isin(processed_ids)]\n",
    "\n",
    "    # Batch processing text descriptions\n",
    "    text_descriptions = [create_text_description(row) for _, row in batch.iterrows()]\n",
    "    text_inputs = clip.tokenize(text_descriptions).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = model.encode_text(text_inputs).cpu().numpy()\n",
    "\n",
    "    # Batch processing images\n",
    "    image_embeddings = []\n",
    "    for _, row in batch.iterrows():\n",
    "        image_id = str(row['id'])\n",
    "        image_path = os.path.join(image_folder, f\"{image_id}.jpg\")\n",
    "\n",
    "        if os.path.exists(image_path):\n",
    "            image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                image_embedding = model.encode_image(image).cpu().numpy()\n",
    "            image_embeddings.append(image_embedding)\n",
    "        else:\n",
    "            print(f\"Image {image_id}.jpg not found, skipping image embedding.\")\n",
    "            image_embeddings.append(None)\n",
    "\n",
    "    # Store embeddings for the current batch\n",
    "    for i, (_,row) in enumerate(batch.iterrows()):\n",
    "        image_id = str(row['id'])\n",
    "        embeddings[image_id] = {\n",
    "            \"text_embedding\": text_embeddings[i],\n",
    "            \"image_embedding\": image_embeddings[i]\n",
    "        }\n",
    "        processed_ids.add(image_id)   #mark the image ID as processed\n",
    "\n",
    "# Save embeddings to a pickle file\n",
    "output_file = \"clip_embeddings_batch.pkl\"\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "print(f\"Embeddings have been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d367b03a-8312-42bb-9873-4dff6dc93843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
