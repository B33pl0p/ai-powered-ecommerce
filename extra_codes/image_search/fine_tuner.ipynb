{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a996181a-9734-4cbd-acc9-7adba69c8868",
   "metadata": {},
   "source": [
    "### !ls /home/saurav/Documents/required_dataset/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1896a3b-7854-4a34-a141-2167f9626146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saurav/base_env/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2025-01-31 14:24:51.631168: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-31 14:24:51.672715: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-31 14:24:52.688788: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import clip\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "#loading CLIP MODEL and preprocessing funciton\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1534b0a-cf4c-44fb-b1f8-4cb4cc340acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/saurav/Documents/'\n",
    "image_folder = DATA_PATH+'required_dataset/images'\n",
    "text_file = DATA_PATH+'required_dataset/styles2.csv'\n",
    "df = pd.read_csv(text_file)\n",
    "total_rows = len(df)\n",
    "batch_size = 4\n",
    "embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52b10ee0-4440-4df4-8bde-79b74a7b0c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_description(row):\n",
    "    columns = [\n",
    "        str(row['gender']),\n",
    "        str(row['masterCategory']),\n",
    "        str(row['subCategory']),\n",
    "        str(row['articleType']),\n",
    "        str(row['baseColour']),\n",
    "        str(row['season']),\n",
    "        str(row['year']),\n",
    "        str(row['usage']),\n",
    "        str(row['productDisplayName'])\n",
    "    ]\n",
    "    return ' '.join(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09baa2ec-4299-439c-82e2-1f7a5f49070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ls /home/saurav/Documents/required_dataset/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbc38bc8-dc5d-44c8-a6f0-ec0968321424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom dataset loader creation\n",
    "class ImageTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, texts, preprocess):\n",
    "        self.image_paths = image_paths\n",
    "        self.texts = texts  #list of the corresponding texts\n",
    "        self.preprocess = preprocess  #The CLIP processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)   #number of samples in the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Warning: image file {image_path} not found. Skipping this image\")\n",
    "            return None\n",
    "            \n",
    "        image = Image.open(image_path)    #open image\n",
    "        text = self.texts[idx]                       #get corresponding text\n",
    "        #inputs = self.preprocess(text=[text], images=image, return_tensors=\"pt\", padding=True)\n",
    "        text_inputs = clip.tokenize([text]).squeeze(0).to(device)\n",
    "        image_inputs = self.preprocess(image).to(device)\n",
    "       \n",
    "        return{'image': image_inputs, 'text': text_inputs}    #return processed image-text pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f203ca3d-8699-4911-ab58-638fee687ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a DataLoader\n",
    "batch = df.iloc[0:total_rows]\n",
    "image_id = [str(row['id']) for _,row in batch.iterrows()]\n",
    "image_paths = [os.path.join(image_folder, f\"{image_id}.jpg\") for image_id in image_id]\n",
    "\n",
    "texts = [generate_description(row) for _, row in batch.iterrows()]\n",
    "\n",
    "dataset = ImageTextDataset(image_paths, texts, preprocess)   #create dataset\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True) #create DataLoader  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4a2839-c5cf-42e1-9cdd-d9eb6a67ed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up training process\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)  \n",
    "loss_fn = nn.CrossEntropyLoss()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ff7db3-2e56-49c4-b0f9-ae1be21c3794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                 | 0/20 [00:00<?, ?it/s]\n",
      "  0%|                                                               | 0/2778 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|                                                     | 1/2778 [00:07<5:33:34,  7.21s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "model.to(device)  #move model to correct model\n",
    "model.train()  #set model to training mode\n",
    "\n",
    "num_epoch = 20\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    for batch in tqdm(dataloader):\n",
    "        if not batch:   #skip empty batch\n",
    "            continue\n",
    "        optimizer.zero_grad()  #reset gradients\n",
    "\n",
    "        inputs = {k: v.to(device) for k,v in batch.items()}  #move batch to GPU/CPU\n",
    "        image_features, text_features = model(**inputs)  #forward pass\n",
    "        \n",
    "        logits_per_image = (image_features @ text_features.T)  #image-text similarity score\n",
    "        logits_per_text = logits_per_image.T  #text-image similarity score\n",
    "\n",
    "        ground_truth = torch.arange(len(logits_per_image), device=device)  #create labels\n",
    "        loss = (loss_fn(logits_per_image, ground_truth) + loss_fn(logits_per_text, ground_truth)) / 2   #compute loss\n",
    "\n",
    "        loss.backward()  #Backpropagation (adjust model weights)\n",
    "        optimizer.step()  #update model parameteres\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8967cd-cea9-49d4-a67b-a40320778c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a0b13a-144f-4615-91ee-6d604dc00613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b273d-771f-4a21-becb-1316c455894a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
